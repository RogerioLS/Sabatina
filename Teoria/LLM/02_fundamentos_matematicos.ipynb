{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivadas Parciais e Gradientes\n",
    "\n",
    "O que s√£o derivadas parciais?\n",
    "\n",
    "Em um modelo de aprendizado de m√°quina, a fun√ß√£o que queremos otimizar √© uma **fun√ß√£o de erro ou perda** (por exemplo, a **fun√ß√£o de perda de entropia cruzada** para classifica√ß√£o). Essa fin√ß√£o depende de v√°rios par√¢metros (como pesos de redes neurais). As **derivadas parciais** medem como a fun√ß√£o de erro muda em rela√ß√£o a cada par√¢metro individualmente, enquanto os outros par√¢metros permanecem constantes.\n",
    "\n",
    "üîπ**Derivadas Parciais** A derivada parcial de uma fun√ß√£o $f(x_1, x_2, \\dots, x_n)$ em rela√ß√£o a $x_i$ √© a taxa de varia√ß√£o de $f$ quando $x_i$ muda, enquanto as outras vari√°veis s√£o mantidas constantes.\\\n",
    "Por exemplo, se temos uma fun√ß√£o de perda $L(w_1, w_2)$, as derivadas parciais seriam:\n",
    "$$\\frac{\\partial L}{\\partial w_1} \\quad \\text{e} \\quad \\frac{\\partial L}{\\partial w_2}$$\n",
    "Aqui,  $w_1$ e $w_2$ s√£o os pesos da rede neural.\n",
    "\n",
    "O que s√£o Gradientes?\n",
    "\n",
    "O gradiente √© um vetor que cont√©m todas as derivadas parciais de uma fun√ß√£o de erro. Quando treinamos uma rede neural, o gradiente nos diz em qual dire√ß√£o e com qual magnitude devemos ajustar os pesos para reduzir a fun√ß√£o de perda.\n",
    "\n",
    "üîπO gradiente de uma fun√ß√£o $f(w_1, w_2, \\dots, w_n)$ √© dado por:\n",
    "$$\\nabla f(w_1, w_2, \\dots, w_n) = \\left( \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\dots, \\frac{\\partial f}{\\partial w_n} \\right)$$\n",
    "\n",
    "Em rede neurais, utiliza o algoritmo de backpropagation usa o gradiente para ajustar os pesos da rede, calculando as derivadas parciais da fun√ß√£o de erro em rela√ß√£o aos pesos e aplicando esses ajustes em cada camada.\n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "Se estamos treinando uma rede neural para prever a pr√≥xima palavra em uma sequ√™ncia de texto (como em um modelo de linguagem), queremos minimizar a fun√ß√£o de perda (por exempl, a perda de entropia cruzada) com rela√ß√£o aos pesos da rede. Isso √© feito calculando o gradiente da fun√ß√£o de erro e atualizando os pesos.\n",
    "\n",
    "üîπDigamos que nossa fun√ß√£o de perda seja $L = (y - \\hat{y})^2$, onde $y$ √© o valor real e $/hat{y}$ √© previs√£o.\\\n",
    "üîπO gradiente de $L$ com rela√ß√£o ao peso $w$ seria:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = 2 (y - \\hat{y}) \\cdot \\frac{\\partial \\hat{y}}{\\partial w}$$\n",
    "\n",
    "Esse gradiente √© ente√£o usado para atualizar os pesos da rede neural durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Fun√ß√£o de erro (fun√ß√£o quadr√°tica)\n",
    "def L(w):\n",
    "    return (w - 2)**2\n",
    "\n",
    "# Derivada da fun√ß√£o de erro\n",
    "def grad_L(w):\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "# Gradiente Descendente\n",
    "def gradient_descent(starting_w, learning_rate, iterations):\n",
    "    w_values = [starting_w]  # Ponto inicial\n",
    "    for i in range(iterations):\n",
    "        grad = grad_L(w_values[-1])\n",
    "        new_w = w_values[-1] - learning_rate * grad  # Atualiza o peso\n",
    "        w_values.append(new_w)\n",
    "    return w_values\n",
    "\n",
    "# Par√¢metros\n",
    "starting_w = 5.0  # Ponto de in√≠cio\n",
    "learning_rate = 0.1  # Taxa de aprendizado\n",
    "iterations = 20  # N√∫mero de itera√ß√µes\n",
    "\n",
    "# Executando o Gradiente Descendente\n",
    "w_values = gradient_descent(starting_w, learning_rate, iterations)\n",
    "\n",
    "# Gerando o gr√°fico\n",
    "fig, ax = plt.subplots()\n",
    "w = np.linspace(0, 4, 100)\n",
    "ax.plot(w, L(w), label=\"Fun√ß√£o de erro (L(w) = (w - 2)^2)\", color=\"blue\")\n",
    "ax.set_xlim(0, 5)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# Ponto de m√≠nimo\n",
    "ax.scatter(2, 0, color=\"red\", label=\"M√≠nimo (w = 2)\")\n",
    "\n",
    "# Anima√ß√£o\n",
    "line, = ax.plot([], [], 'ro', label='Progresso do Gradiente')\n",
    "\n",
    "def update(frame):\n",
    "    line.set_data(w_values[:frame], L(np.array(w_values[:frame])))\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(w_values), interval=500, repeat=False)\n",
    "\n",
    "# Exibindo o gr√°fico animado\n",
    "plt.legend()\n",
    "plt.title(\"Gradiente Descendente para Minimizar L(w)\")\n",
    "plt.show()\n",
    "\n",
    "# Para salvar a anima√ß√£o em um arquivo gif, podemos usar:\n",
    "# ani.save('gradient_descent.gif', writer='imagemagick', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
