### **Data Prep**

#### `Tratamento de missing`

Antes mesmo de entender o que Ã© tratamento de missing temos que entender o que pode gerar esses dados ou do porque existem missing?
Dando uma resposta bem pragmÃ¡tica, os missing podem acontecer por dois fatores, `erro tÃ©cnico` ou `erro humano`.
Lendo um [medium](https://medium.com/datapsico/valores-missing-parte-1-4382bb026660) bem interessante falando do tipos de missing:

- `Missings completamente aleatÃ³rios (MCAR)`
Os completamente aleatÃ³rios ocorreram por erro tÃ©cnico ou obra do acaso. Algo que se perdeu no caminho e do qual nÃ£o fazemos ideia do porquÃª.
- `Missings aleatÃ³rios (MAR)`
Os aleatÃ³rios existem porque a existÃªncia de uma outra variÃ¡vel aumenta a probabilidade dessa resposta nÃ£o existir.
- `Missings nÃ£o aleatÃ³rios (MNAR)`
Os nÃ£o aleatÃ³rios nÃ£o sÃ£o aleatÃ³rios - existem porque a variÃ¡vel em si Ã© a causa de nÃ£o existir um escore dela mesma.

Com isso esses tipos de missings podem ser importante para entendermos como fazermos uma boa imputaÃ§Ã£o desses valores faltantes seja em nosso banco de dados ou em nosso modelos.

> [!IMPORTANT]  
> VocÃª sabia?
> 
> - NA significa â€œNo answerâ€ (â€œSem respostaâ€) ou â€œNot applicableâ€ (â€œNÃ£o se aplicaâ€).

Voltando para a nossa abordagem inicial, como podemos tratar missings?
Na literatura ou no nosso dia a dia nÃ£o existe uma bala de prata que vÃ¡ trazer a soluÃ§Ã£o perfeita para todos os casos, por isso vale muito aplicar todos os mÃ©todos possÃ­veis e ir avaliando qual melhor se encaixa para o contexto da analise ou modelo de ML, por isso vou sitar alguns:
- `RemoÃ§Ã£o de linhas:` Excluir todas as linhas que contÃªm valores missing. Talvez seja Ãºtil quando a quantidade de dados faltantes Ã© pequena.
- `RemoÃ§Ã£o de colunas:` Excluir colunas inteira se a maioria dos valores estiverem faltando.
- `ImputaÃ§Ã£o com valor constante medidas de tendÃªncia central e medidas de dispersÃ£o:` Substitui valores missing por uma constante, como zero ou a mÃ©dia/mediana/moda, amplitude(A)/intervalo-interquartil/variÃ¢ncia/desvio-padrÃ£o/coeficiente de variaÃ§Ã£o.
- `ImputaÃ§Ã£o baseada em k-vizinhos mais prÃ³ximos (KNN):` Usa valores dos k-vizinhos mais prÃ³ximos para imputar valores missing.
- `ImputaÃ§Ã£o multivariada (MICE):` Usa modelos estatÃ­sticos para prever valores missing com base em outras variÃ¡veis.
- `InterpolaÃ§Ã£o:` Usa mÃ©todos de interpolaÃ§Ã£o linear ou polinomial para preencher valores missing em sÃ©ries temporais.

Essas sÃ£o os possÃ­veis mÃ©todos que podemos utilizar para se tratar os missings, lembrando que podem existir casos que ainda nÃ£o foram listados nesse resumo, e que o tratamento de missing vai de acordo com o que vocÃª estÃ¡ modelando.

#### `Tratamento de outliers`

Um ponto importante Ã© que antes de falarmos de tratamento de outliers, vale dizer o que sÃ£o outliers?
Em uma definiÃ§Ã£o bem abrangente podemos dizer que outliers, sÃ£o observaÃ§Ãµes ou pontos de dados que se encontram significativamente afastados da maioria dos dados em um conjunto de dados. Eles podem surgir devido a variabilidade inerente ao sistema, erros de mediÃ§Ã£o ou colta de dados, ou devido Ã¡ presenÃ§a de fenÃ´menos anÃ´malos. A identificaÃ§Ã£o de outliers Ã© crucial, pois eles podem distorcer a anÃ¡lise estatÃ­stica e influenciar significativamente os resultados de modelos de machine learning.

Seguindo essa lÃ³gica de rastreabilidade de outliers temos os tipos:
- `Univariados:` Anomalias que ocorrem em uma Ãºnica variÃ¡vel.
- `Multivariados:` Anomalias que ocorrem em um contexto multidimensional.
- `Artificiais:` Anomalias que ocorrem em falhas de mediÃ§Ã£o, inferÃªncia erradas, ou falhas de processamento de dado.
- `Naturais:` Anomalias que ocorrem quando o dado Ã© somente uma exceÃ§Ã£o atÃ­pica e discrepante em relaÃ§Ã£o ao conjunto de observaÃ§Ãµes.

> [!IMPORTANT]  
> Essas sÃ£o possÃ­veis caracterÃ­sticas de outliers:
> - ``DistÃ¢ncia:`` Eles estÃ£o a uma distÃ¢ncia considerÃ¡vel dos outros pontos de dados.
> - `Raridade:` SÃ£o raros ou incomuns dentro do conjunto de dados.
> - `InfluÃªncia:` Podem ter um impacto desproporcional em estatÃ­sticas como mÃ©dia e variÃ¢ncia.

Pensando na nossa abordagem principal, como podemos realizar tratamento de outliers?
VocÃª vai me ver falar muito de frase `NÃ£o existe bala de prata` tudo depende do contexto que vocÃª estÃ¡ trabalhando, entÃ£o o que sempre sugiro Ã© testar.

E para se tratar os outliers encontrado na sua base de dados, aqui temos alguns mÃ©todos utilizados.

- `Mediana:` Substituir outliers pela mediana dos dados.
```python
median = df['feature'].median()
df['feature'] = np.where(np.abs(stats.zscore(df['feature'])) > 3, median, df['feature'])

```
- `Desvio PadrÃ£o:` Essa medida de dispersÃ£o de um conjunto de dados diz que quanto menor o desvio padrÃ£o mais homogÃªneos sÃ£o os dados e ao contrÃ¡rio disso quanto maior o desvio mais os dados espalhados estÃ£o.
Outra informaÃ§Ã£o importante Ã© sobre o teorema das probabilidades que afirma que quanto maior a amostra, mais a distribuiÃ§Ã£o amostral da sua mÃ©dia aproxima-se de uma distribuiÃ§Ã£o normal.
Quando consideramos valores que sÃ£o distribuÃ­dos de acordo com a curva de Gauss podemos fazer algumas inferÃªncias importantes para tratarmos os dados. Uma dessas inferÃªncias Ã© que 96,6% dos dados estÃ£o contidos em 3 desvios padrÃµes e o que estÃ¡ fora desse intervalo de confianÃ§a pode ser considerado como sendo um outlier.
<div align="center">

![Desvio PadrÃ£o](https://miro.medium.com/v2/resize:fit:300/1*RULOf2Y0yGyqbb09V8xMjQ.png)

</div>

- `Z-score:` nos diz o quanto cada valor em nossa distribuiÃ§Ã£o se distancia da mÃ©dia em termos de desvio padrÃ£o.

<div align="center">

![Z-score](https://miro.medium.com/v2/resize:fit:219/1*Af8Wm1o8y7iBJjOGTPXO2A.png)![Z-score-imple](https://miro.medium.com/v2/resize:fit:449/1*KZja2D6znAtjxvxeOJN2Tg.png)

</div>

- `Desvio absoluto mediano:` um conjunto de dados Ã© a distÃ¢ncia entre cada dado em relaÃ§Ã£o a mediana.

<div align="center">

![MDAM](https://miro.medium.com/v2/resize:fit:361/1*rP_fYzL3F3Ztr_Cxify4Uw.png)![MDAM_imple](https://miro.medium.com/v2/resize:fit:567/1*Wne1ME1Q2Yx8gxctLdOueA.png)

</div>

- `MÃ©todo de Tukey:` o mÃ©todo de Tukey ou bloxplot consiste em definir os limites inferior e superior a partir do interquartil (IQR) e dos primeiros (Q1) e terceiros (Q3) quartis.
Os quartis sÃ£o separatrizes que dividem um conjunto de dados em 4 partes iguais. O objetivo das separatrizes Ã© proporcionar uma melhor ideia da dispersÃ£o do conjunto de dados, principalmente da simetria ou assimetria da distribuiÃ§Ã£o.
O limite inferior Ã© definido pelo primeiro quartil menos o produto entre o valor 1.5 e o interquartil.
$$ğ¿ğ‘–ğ‘›ğ‘“ = ğ‘„1 âˆ’ (1.5 âˆ— ğ¼ğ‘„ğ‘…)$$
O limite superior Ã© definido pelo terceiro quartil mais o produto entre o valor 1.5 e o interquartil.
$$ğ¿ğ‘ ğ‘¢ğ‘ = ğ‘„3 + (1.5 âˆ— ğ¼ğ‘„ğ‘…)$$

<div align="center">

![quartil_imple](https://miro.medium.com/v2/resize:fit:490/1*W8E3Ek_dmaISZBOyTAmrMA.png)

</div>

- `Isolation Forest:` Foi o primeiro algoritmo de detecÃ§Ã£o de anomalias usando a estratÃ©gia de isolamento.
Esse algoritmo Ã© basicamente uma floresta aleatÃ³ria em que cada arvore de decisÃ£o Ã© cultivada aleatoriamente. As arvores irÃ£o dividir e subdividir os dados baseado em um valor aleatÃ³rio de corte atÃ© que todos os dados eventualmente estejam todos cortados e separados. Os dados mais discrepantes serÃ£o isolados mais rapidamente do que os demais, podendo assim ser identificado como um outlier.

<div align="center">

![isolation](https://miro.medium.com/v2/resize:fit:511/1*WSnHJrbeZefheqjxbno2jw.png)
![isolation_imple](https://miro.medium.com/v2/resize:fit:461/1*ODyLL3OaDe_KPX6rjD0poQ.png)
</div>


#### `CategorizaÃ§Ã£o de variÃ¡veis contÃ­nuas e discretas`

CategorizaÃ§Ã£o para essas tipos de variÃ¡veis sÃ£o o processo de transformar variÃ¡veis contÃ­nuas ou discretas em variÃ¡veis categÃ³ricas.

VariÃ¡veis contÃ­nuas sÃ£o aquelas que podem assumir qualquer valor dentro de um intervalo, com por exemplo altura, peso, temperatura e tempo. Formas para se categorizar variÃ¡veis contÃ­nuas:
- `Binning (DiscretizaÃ§Ã£o):` Ã‰ o processo de dividir a faixa de valores contÃ­nuos em intervalos (bins) e atribuir um rÃ³tulo categÃ³rico a cada intervalo.
- `Equal-width Binning:` Divide os dados em intervalos de igual largura.
- `Equal-frequency Binning:` Divide os dados de modo que cada intervalo contenha aproximadamente o mesmo nÃºmero de pontos de dados.
- `Custom Binning:` Utiliza intervalos definidos pelo usuÃ¡rio com base no conhecimento do domÃ­nio ou caracterÃ­sticas dos dados.

```python
import pandas as pd

# Supondo um DataFrame com uma variÃ¡vel contÃ­nua 'idade'
df = pd.DataFrame({'idade': [23, 45, 12, 67, 34, 25, 37, 50, 29, 61]})

# Equal-width Binning
df['idade_binned'] = pd.cut(df['idade'], bins=3, labels=["Jovem", "Adulto", "Idoso"])

# Equal-frequency Binning
df['idade_binned_freq'] = pd.qcut(df['idade'], q=3, labels=["Jovem", "Adulto", "Idoso"])

# Custom Binning
bins = [0, 18, 35, 50, 100]
labels = ["CrianÃ§a/Adolescente", "Jovem Adulto", "Adulto", "Idoso"]
df['idade_custom_binned'] = pd.cut(df['idade'], bins=bins, labels=labels)

```

VariÃ¡veis discretas sÃ£o aquelas que assumem valores distintos e finitos, como o nÃºmero de filhos, contagem de produtos vendidos, ou nÃºmero de eventos. Uma forma para se categorizar variÃ¡veis discretas:
- `Agrupamento:` Agrupar valores discretos em categorias mais amplas. Pode ser feito de forma similar ao binning para variÃ¡veis contÃ­nuas, mas geralmente com base em lÃ³gica de negÃ³cio ou conhecimento do domÃ­nio.
```python
# Supondo um DataFrame com uma variÃ¡vel discreta 'num_filhos'
df = pd.DataFrame({'num_filhos': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})

# Agrupamento customizado
bins = [0, 2, 4, 6, 10]
labels = ["Nenhum ou Poucos", "Alguns", "Muitos", "Grandes FamÃ­lias"]
df['num_filhos_binned'] = pd.cut(df['num_filhos'], bins=bins, labels=labels)
```

> [!IMPORTANT]  
> VariÃ¡veis quantitativas podem ser classificadas como discretas ou contÃ­nuas:
> - `VariÃ¡veis Discretas:` Assumem valores especÃ­ficos e contÃ¡veis (ex.: nÃºmero de filhos).
> - `VariÃ¡veis ContÃ­nuas:` Podem assumir qualquer valor dentro de um intervalo (ex.: altura, peso).
> - `VariÃ¡veis CategÃ³ricas:` Representam categorias ou grupos:
>	- `Nominais:` Sem ordem (ex.: cor dos olhos).
>	- `Ordinais:` Com ordem (ex.: classificaÃ§Ã£o de serviÃ§o).

#### `PCA`

A AnÃ¡lise de Componentes Principais, ou PCA, Ã© uma tÃ©cnica fundamental no domÃ­nio da anÃ¡lise de dados e aprendizado de mÃ¡quina.
O PCA Ã© um mÃ©todo estatÃ­stico que transforma dados de alta dimensÃ£o em um formato de menor dimensÃ£o, preservando as informaÃ§Ãµes mais importantes. Isto Ã© conseguido atravÃ©s da identificaÃ§Ã£o de novos eixos, chamados componentes principais, ao longo dos quais os dados variam mais. Esses componentes sÃ£o ortogonais entre si, o que significa que nÃ£o estÃ£o correlacionados, o que os torna uma ferramenta poderosa para reduÃ§Ã£o de dimensionalidade.

E quando podemos utilizar esse mÃ©todo:
Imagine que vocÃª tem um conjunto de dados com muitos recursos ou variÃ¡veis. Cada recurso contribui para a complexidade geral dos dados, tornando difÃ­cil analisar, visualizar ou construir modelos. A alta dimensionalidade pode levar a vÃ¡rios problemas:

- `Complexidade Computacional:` Ã€ medida que o nÃºmero de recursos aumenta, os recursos computacionais e o tempo necessÃ¡rio para anÃ¡lise e modelagem crescem exponencialmente.
- `Overfitting:` Os modelos podem se tornar excessivamente complexos e ajustar o ruÃ­do nos dados, levando a uma generalizaÃ§Ã£o deficiente em dados novos e nÃ£o vistos.
- `Dificuldade de visualizaÃ§Ã£o:` Torna-se um desafio visualizar e compreender dados em mais de trÃªs dimensÃµes.
- `RedundÃ¢ncia:` alguns recursos podem estar altamente correlacionados, o que significa que transmitem informaÃ§Ãµes semelhantes. Essa redundÃ¢ncia pode ser eliminada sem perda significativa de informaÃ§Ãµes.

Seguindo o raciocÃ­nio leva trÃªs etapas para se fazer o calculo:
- `PadronizaÃ§Ã£o de Dados:` antes de realizar o PCA, Ã© fundamental padronizar os dados. Isso significa centralizar os dados subtraindo a mÃ©dia e escalÃ¡-los dividindo pelo desvio padrÃ£o. A padronizaÃ§Ã£o garante que todos os recursos tenham igual importÃ¢ncia na anÃ¡lise.

- `Matriz de CovariÃ¢ncia:` o PCA depende do cÃ¡lculo da matriz de covariÃ¢ncia. A covariÃ¢ncia entre duas variÃ¡veis â€‹â€‹mede como elas mudam juntas. A matriz de covariÃ¢ncia para um conjunto de dados com n recursos Ã© uma matriz nxn que resume as relaÃ§Ãµes entre todos os pares de recursos.

- `cÃ¡lculo de autovalor e autovetor:` a prÃ³xima etapa Ã© calcular os autovalores e autovetores da matriz de covariÃ¢ncia. Esses autovalores representam a quantidade de variÃ¢ncia explicada por cada autovetor (componente principal). Autovalores e autovetores sÃ£o conceitos matemÃ¡ticos relacionados a transformaÃ§Ãµes lineares e matrizes. No contexto da PCA, desempenham um papel central na identificaÃ§Ã£o dos componentes principais. Aqui estÃ¡ o que eles significam:
	- `Autovalor:` Um autovalor (Î») representa um escalar que indica quanta variÃ¢ncia Ã© explicada pelo autovetor correspondente. No PCA, os autovalores quantificam a importÃ¢ncia de cada componente principal. SÃ£o sempre nÃ£o negativos e o autovalor correspondente a um componente principal mede a proporÃ§Ã£o da variÃ¢ncia total nos dados explicada por esse componente.
	- `Autovetor:` Um autovetor (v) Ã© um vetor associado a um autovalor. No PCA, os autovetores representam as direÃ§Ãµes ao longo das quais os dados mais variam. Cada autovetor aponta em uma direÃ§Ã£o especÃ­fica no espaÃ§o de recursos e corresponde a um componente principal. Os autovetores sÃ£o normalmente normalizados, o que significa que seu comprimento Ã© 1.


Para trazer mais clareza vamos calcular o PCA (AnÃ¡lise de Componentes Principais) passo a passo usando um exemplo simples. Suponha que temos um pequeno conjunto de dados com duas variÃ¡veis (X e Y):

\[
\begin{array}{|c|c|c|}
\hline
   & X  & Y  \\
\hline
 1 & 2  & 4  \\
 2 & 3  & 5  \\
 3 & 5  & 7  \\
 4 & 6  & 8  \\
 5 & 8  & 9  \\
\hline
\end{array}
\]


`Passo 1: Centralizar os dados`

Primeiro, centralizamos os dados subtraindo a mÃ©dia de cada variÃ¡vel.

- MÃ©dia de X: (2 + 3 + 5 + 6 + 8) / 5 = 4.8
- MÃ©dia de Y: (4 + 5 + 7 + 8 + 9) / 5 = 6.6

\[
\begin{array}{|c|c|c|c|c|}
\hline
   & X & Y & \text{X Centralizado} & \text{Y Centralizado} \\
\hline
1 & 2 & 4 & 2 - 4.8 = -2.8 & 4 - 6.6 = -2.6 \\
2 & 3 & 5 & 3 - 4.8 = -1.8 & 5 - 6.6 = -1.6 \\
3 & 5 & 7 & 5 - 4.8 =  0.2 & 7 - 6.6 =  0.4 \\
4 & 6 & 8 & 6 - 4.8 =  1.2 & 8 - 6.6 =  1.4 \\
5 & 8 & 9 & 8 - 4.8 =  3.2 & 9 - 6.6 =  2.4 \\
\hline
\end{array}
\]


`Passo 2: Calcular a matriz de covariÃ¢ncia`

A matriz de covariÃ¢ncia Ã© calculada a partir dos dados centralizados.

\[ 
\begin{bmatrix}
\text{Cov(X, X)} & \text{Cov(X, Y)} \\
\text{Cov(Y, X)} & \text{Cov(Y, Y)}
\end{bmatrix}
\]

Para este exemplo, a matriz de covariÃ¢ncia Ã©:

- Cov(X, X) = \(\frac{1}{4} \sum_{i=1}^{5} (X_i - \bar{X})^2\)
- Cov(X, Y) = \(\frac{1}{4} \sum_{i=1}^{5} (X_i - \bar{X})(Y_i - \bar{Y})\)
- Cov(Y, Y) = \(\frac{1}{4} \sum_{i=1}^{5} (Y_i - \bar{Y})^2\)

\[
\begin{array}{|c|c|c|c|c|}
\hline
\text{X Centralizado} & \text{Y Centralizado} & (X - \bar{X})(X - \bar{X}) & (X - \bar{X})(Y - \bar{Y}) & (Y - \bar{Y})(Y - \bar{Y}) \\
\hline
-2.8 & -2.6 & 7.84 & 7.28 & 6.76 \\
-1.8 & -1.6 & 3.24 & 2.88 & 2.56 \\
 0.2 &  0.4 & 0.04 & 0.08 & 0.16 \\
 1.2 &  1.4 & 1.44 & 1.68 & 1.96 \\
 3.2 &  2.4 & 10.24 & 7.68 & 5.76 \\
\hline
\text{Total} & \text{Total} & 22.8 & 19.6 & 17.2 \\
\hline
\end{array}
\]

- Cov(X, X) = $\frac{22.8}{4} = 5.7$
- Cov(X, Y) = $\frac{19.6}{4} = 4.9$
- Cov(Y, Y) = $\frac{17.2}{4} = 4.3$

Matriz de covariÃ¢ncia:

\[ 
\begin{bmatrix}
5.7 & 4.9 \\
4.9 & 4.3
\end{bmatrix}
\]

`Passo 3: Calcular os autovalores e autovetores`

Calculamos os autovalores e autovetores da matriz de covariÃ¢ncia.

- Autovalores: Î»1 = 10.1, Î»2 = 0
- Autovetores correspondentes:

\[ 
\mathbf{v_1} = \begin{bmatrix}
0.79 \\
0.61
\end{bmatrix}
\]

\[ 
\mathbf{v_2} = \begin{bmatrix}
-0.61 \\
0.79
\end{bmatrix}
\]

`Passo 4: Projetar os dados nos componentes principais`

Multiplicamos os dados centralizados pelos autovetores para obter as componentes principais.

\[
\begin{bmatrix}
\text{PC1} \\
\text{PC2}
\end{bmatrix}=
\begin{bmatrix}
0.79 & 0.61 \\
-0.61 & 0.79
\end{bmatrix}
\begin{bmatrix}
X_{\text{centralizado}} \\
Y_{\text{centralizado}}
\end{bmatrix}
\]

Calculando para cada ponto:

- Para (2, 4): PC1 = 0.79*(-2.8) + 0.61*(-2.6) = -3.6
- Para (3, 5): PC1 = 0.79*(-1.8) + 0.61*(-1.6) = -2.3
- Para (5, 7): PC1 = 0.79*(0.2) + 0.61*(0.4) = 0.5
- Para (6, 8): PC1 = 0.79*(1.2) + 0.61*(1.4) = 2.1
- Para (8, 9): PC1 = 0.79*(3.2) + 0.61*(2.4) = 5.3

Assim, temos os dados projetados nos novos componentes principais.

> [!IMPORTANT]  
> Vale lembrar que todo essa tratamento pode acaber gerando o que chamamos de [MaldiÃ§Ã£o da dimensionalidade](https://medium.com/data-hackers/maldi%C3%A7%C3%A3o-da-dimensionalidade-655e4342d64).
> A MaldiÃ§Ã£o da Dimensionalidade foi denominada pelo matemÃ¡tico R. Bellman em seu livro â€œProgramaÃ§Ã£o DinÃ¢micaâ€ em 1957. Segundo ele, a maldiÃ§Ã£o da dimensionalidade Ã© o problema causado pelo aumento exponencial do volume associado Ã  adiÃ§Ã£o de dimensÃµes extras ao espaÃ§o euclidiano.

![Aumento de dimensÃµes gera dados mais esparsos](https://miro.medium.com/v2/resize:fit:875/1*FxCli5YAOuMPxHR_Xwe1xg.png)


#### `CorrelaÃ§Ã£o / associaÃ§Ã£o entre dados contÃ­nuos e entre dados discretos`

`CorrelaÃ§Ã£o x AssociaÃ§Ã£o`
De forma simples podemos pensar nas duas como a mesma â€œcoisaâ€, porÃ©m estatisticamente nÃ£o Ã© â€œcorretoâ€. Quando falamos do relacionamento entre variÃ¡veis numÃ©ricas buscamos correlaÃ§Ã£o e quando falarmos sobre categÃ³ricas e numÃ©ricas e/ou categÃ³ricas e categÃ³ricas buscamos associaÃ§Ã£o.
Quando estamos avaliando duas variÃ¡veis, temos que lembrar que Ã© conhecida como anÃ¡lise bivariada.
- AnÃ¡lise da relaÃ§Ã£o entre duas variÃ¡veis, conhecida como anÃ¡lise bivariada;
- Medidas de correlaÃ§Ã£o para anÃ¡lise da relaÃ§Ã£o entre variÃ¡veis numÃ©ricas;
- Medidas de associaÃ§Ã£o para anÃ¡lise da relaÃ§Ã£o entre variÃ¡veis categÃ³ricas;

Um desenho ilustrativo para auxiliar o entendimento do fluxo de medidas de acordo com os dados observados.

![](https://miro.medium.com/v2/resize:fit:875/1*QGpZwlAnOvHP0PY_VIM_8A.png)

`Medidas de correlaÃ§Ã£o`

- `CovariÃ¢ncia:` como medida de variaÃ§Ã£o conjunta entre variÃ¡veis numÃ©ricas.
A covariÃ¢ncia Ã© calculada pela fÃ³rmula cov(X,Y) = Î£(xi â€“ xÌ„)(yi â€“ È³) / (n-1), levando em conta os desvios de cada observaÃ§Ã£o em relaÃ§Ã£o Ã  mÃ©dia, medindo o quanto esses desvios variam conjuntamente entre as duas variÃ¡veis.

- `Coeficiente de correlaÃ§Ã£o de Pearson:` provavelmente o mais familiar para nÃ³s Ã© a correlaÃ§Ã£o de Pearson, que tem como objetivo medir o grau de correlaÃ§Ã£o entre duas variÃ¡veis que possuem uma relaÃ§Ã£o linear. <div align="center"> ![](https://miro.medium.com/v2/resize:fit:743/1*4KfFijGsdKSBUku77FTVmg.png) </div>
O r varia de -1 atÃ© 1, onde 1 Ã© uma correlaÃ§Ã£o positiva perfeita, -1 correlaÃ§Ã£o negativa perfeita e 0 nÃ£o existe nenhuma correlaÃ§Ã£o linear entre as variÃ¡veis, abaixo desenhei alguns exemplos para que fique mais claro: <div align="center"> ![](https://miro.medium.com/v2/resize:fit:875/1*FRBKVBFxn2a2dIgOlzbwpw.png) </div>
Supondo que encontramos uma correlaÃ§Ã£o de pearson de -1, isso significa dizer que a medida que a variÃ¡vel x aumenta a variÃ¡vel y diminui proporcionalmente, ou seja uma influencia negativamente na outra. 
	```python
	# CorrelaÃ§Ã£o de pearson entre duas variÃ¡veis 
	import pandas as pd
	df[['coluna X','coluna Y']].corr()
	```

- `CorrelaÃ§Ã£o de Spearman:` Spearman Ã© basicamente uma variaÃ§Ã£o de pearson, com diferenÃ§a na forma como calculamos, tendo basicamente que ordenar as observaÃ§Ãµes criando um novo â€˜datasetâ€™ antes de calcular o coeficiente.
Spearman Ã© o que chamamos de nÃ£o paramÃ©trico ou seja diferente da regressÃ£o linear nÃ£o pressupÃµe nada sobre a distribuiÃ§Ã£o dos dados. <div align="center"> ![](https://miro.medium.com/v2/resize:fit:405/1*ywLSUs2QbR4OmlV5YWz34A.png) </div>
	```python
	# CorrelaÃ§Ã£o de spearman entre duas variÃ¡veis 
	# corr(method='spearman')
	import pandas as pd
	df[['coluna X','coluna Y']].corr(method='spearman')
	```

`Medidas de associaÃ§Ã£o`

- `Coeficiente de CramÃ©r's V:` Utilizaremos o Cramersâ€™V, quando estivermos falando da associaÃ§Ã£o entre duas variÃ¡veis categÃ³ricas.
O resultado advindo do teste Cramersâ€™V , varia de 0 atÃ© 1, onde 0 indica que nÃ£o existe nenhuma associaÃ§Ã£o entre as variÃ¡veis, enquanto 1 indica uma associaÃ§Ã£o perfeita.

  \[
  V = \sqrt{\frac{\chi^2}{n \times \min(k-1, r-1)}}
  \]

  Onde \(\chi^2\) Ã© o valor do teste qui-quadrado, \(n\) Ã© o tamanho da amostra, \(k\) Ã© o nÃºmero de colunas e \(r\) Ã© o nÃºmero de linhas.
  - \( V = 0 \): Nenhuma associaÃ§Ã£o.
  - \( V = 1 \): AssociaÃ§Ã£o perfeita.
	```python
	from scipy.stats.contingency import association
	#1- Criar tabela de contingÃªncia:
	df_cont_cat = pd.crosstab(df['tech_company'],df['treatment'])
	association(df_cont_cat, method="cramer")
	```

- `Coeficiente de ContingÃªncia:`
  \[
  C = \sqrt{\frac{\chi^2}{\chi^2 + n}}
  \]
Onde \(\chi^2\) Ã© o valor do teste qui-quadrado e \(n\) Ã© o tamanho da amostra.
Varia entre 0 e 1, onde valores mais prÃ³ximos de 1 indicam uma associaÃ§Ã£o mais forte.

`CorrelaÃ§Ã£o entre Dados ContÃ­nuos e Discretos`

Quando temos uma variÃ¡vel contÃ­nua e uma variÃ¡vel discreta, o mÃ©todo mais comum Ã© usar ``ANOVA`` (AnÃ¡lise de VariÃ¢ncia) ou a `correlaÃ§Ã£o ponto-bisserial`.

- `Ponto-Bisserial:` o coeficiente de correlaÃ§Ã£o ponto-bisserial mede a associaÃ§Ã£o entre uma variÃ¡vel nÃºmerica e contÃ­nua, por exemplo: DoenÃ§a (Sim ou NÃ£o) em relaÃ§Ã£o ao nÃ­vel de aÃ§ucar no sangue. Ela varia de -1 a 1 seguindo a mesma lÃ³gica de interpretaÃ§Ã£o das anteriores. <div align="center"> ![](https://miro.medium.com/v2/resize:fit:689/0*KkfWBoYkfe2vHi9w.png) </div>
	```python
	from scipy import stats
	#precisamos converter a coluna de qualidade de string para nÃºmero, vamos colocar 1 -good e 0 -bad
	df2['Quality_encoded'] = np.where(df2['Quality']=='good',1,0)
	stats.pointbiserialr(df2['Quality_encoded'], df2['Weight'])
	```

- `AnÃ¡lise de VariÃ¢ncia (ANOVA):` A anÃ¡lise de variÃ¢ncia mais conhecida como ANOVA Ã© um teste estatÃ­stico bastante utilizado nas pesquisas, seja em Ã¡reas biolÃ³gicas e de saÃºde ou exatas. Esse teste estatÃ­stico tem como objetivo verificar a diferenÃ§a entre mÃ©dias de trÃªs ou mais grupos baseado na anÃ¡lise das variÃ¢ncias amostrais.
Ou seja, a ANOVA permite identificar se esses grupos possuem diferenÃ§as estatÃ­sticas significativas ou nÃ£o a partir da comparaÃ§Ã£o de vÃ¡rios grupos que avalia se hÃ¡ diferenÃ§as estatisticamente significativas entre as mÃ©dias de trÃªs ou mais grupos (categorias da variÃ¡vel discreta).
Nesse exemplo temos a variÃ¡vel explanatÃ³ria categÃ³rica (X) e a variÃ¡vel resposta contÃ­nua (Y). Observe na tabela abaixo os dados do experimento:  <div align="center"> ![](https://blog.proffernandamaciel.com.br/wp-content/uploads/2022/09/anova1.png) 
**Tabela 1 â€“ Dados referente a variÃ¡vel comprimento (cm) dividida por quatro tratamentos de Ã¡gua salina com concentraÃ§Ãµes:
T4: 25% de Ã¡gua salina, T3: 50% de Ã¡gua salina, T2: 75% de Ã¡gua salina e T1 apenas com Ã¡gua de torneira.** </div>
Realizando todos os cÃ¡lculos que estÃ£o no quadro 1, temos a seguinte resposta:  <div align="center"> ![](https://blog.proffernandamaciel.com.br/wp-content/uploads/2022/09/anova2.png)
**Tabela 2 â€“ Resultado do cÃ¡lculo de anÃ¡lise de variÃ¢ncia (ANOVA)** </div>
ApÃ³s realizamos os cÃ¡lculos da anÃ¡lise de variÃ¢ncia, o que temos que fazer Ã© visitar nossas hipÃ³teses e verificar qual se hipÃ³tese vamos rejeitar ou aceitar. As hipÃ³teses sÃ£o:
H0: NÃ£o existe diferenÃ§a entre os grupos (p-valor > 0,05).
H1: Existe diferenÃ§a em pelo menos dois grupos (p-valor < 0,05)
Como o p-valor foi 0,38 nÃ£o rejeitamos a hipÃ³tese H0 que informa que nÃ£o existem diferenÃ§as entre os grupos (p-valor > 0,05). Ou seja, as diferentes concentraÃ§Ãµes de Ã¡gua salina nÃ£o influenciaram no comprimento das plantas.

> [!IMPORTANT]  
> **Cuidado com CorrelaÃ§Ãµes e AssociaÃ§Ãµes espÃºrias !**
> Aqui fica outro tema de atenÃ§Ã£o sobre correlaÃ§Ãµes, em alguns momentos podemos nos deparar com correlaÃ§Ãµes entre variÃ¡veis, inclusive significativas, que ocorrem apenas por um acaso e com o mÃ­nimo de anÃ¡lise e contexto fica nÃ­tido que sÃ£o absurdas. [Este site exemplifica alguns casos](http://www.tylervigen.com/spurious-correlations).

#### `SeleÃ§Ã£o de variÃ¡veis`

Para nÃ£o ficar algo redundante e trazer a mesma coisa que foi visto acima, pois seleÃ§Ã£o de variÃ¡veis estÃ¡ totalmente conectado com correlaÃ§Ã£o e associaÃ§Ã£o das variÃ¡veis.
Vou trazer alguns mÃ©todos utilizando o sklearn, Ã© bom lembrar que muitas dessas tÃ©cnicas dependem inicialmente do algoritmo que vocÃª quer usar. Por exemplo, uma feature pode ser considerada importante para um modelo que considera relacionamentos lineares â€” LinearSVC, RegressÃ£o Linear â€” , ao mesmo tempo em que nÃ£o Ã© importante para um modelo que consegue identificar relacionamentos nÃ£o lineares â€” como Decision Trees, Random Forest, etc.

Se tiver dÃºvida sobre qual estimador utilizar para seu modelo, [recomendo dar uma consultada nessa cheat sheet do scikit-learn](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).

- `Sklearn SelectKBest:` Selecionar features pode ser feito atravÃ©s de testes estatÃ­sticos univariados, como na funÃ§Ã£o SelectKBest do sklearn. Esta funÃ§Ã£o seleciona as K melhores features do dataset com base em um teste estatÃ­stico. No exemplo do dataset iris, podemos usar SelectKBest para selecionar as 2 melhores features:
	```python
	from sklearn.datasets import load_iris
	from sklearn.feature_selection import chi2, SelectKBest
	data = load_iris()
	X = data.data
	y = data.target
	X = SelectKBest(chi2, k=2).fit_transform(X, y)
	```
	ApÃ³s esse processo, o dataset terÃ¡ apenas as duas features com melhor pontuaÃ§Ã£o no teste chi2. SelectKBest tambÃ©m funciona para problemas de regressÃ£o com testes como f_regression e mutual_info_regression. No entanto, escolher o nÃºmero ideal de K pode ser difÃ­cil e empÃ­rico. O SelectPercentile, uma alternativa, seleciona um percentual X% das melhores features.
	[Saiba mais sobre SelectKBest na documentaÃ§Ã£o do sklearn](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)

- `Sklearn RFE:` O resultado dele Ã© bem consistente, e seu principal trade-off â€” o tempo â€” nÃ£o chega a ser uma fator tÃ£o negativo em meus projetos.
Assim como seu nome diz â€” Recursive Feature Elimination â€” , o RFE funciona da seguinte forma: **ele irÃ¡ treinar seu modelo utilizando todo seu conjunto inicial**, com todas as features e data points que vierem nele. **ApÃ³s o primeiro treino, o RFE irÃ¡ verificar a importÃ¢ncia das features** â€” utilizando atributos como `coef_` ou `feature_importances_` â€” **e, recursivamente, irÃ¡ remover as features menos importantes** do dataset e treinar o modelo novamente. Ele farÃ¡ isso atÃ© chegar a um nÃºmero ideal de features. Veja abaixo uma aplicaÃ§Ã£o do RFE, onde informo que quero remover uma feature de cada vez. Ou seja, cada vez que o modelo for treinado, ele irÃ¡ remover uma feature. O parÃ¢metro `n_features_to_selectpode` ser passado para informar a quantidade de features que quer selecionar. Se ele for nulo, o RFE escolherÃ¡ metade do total de features.
	```python
	from sklearn.datasets import load_iris
	from sklearn.svm import LinearSVC
	from sklearn.feature_selection import RFEdata = load_iris()
	X = data.data
	y = data.target
	model = LinearSVC()
	rfe = RFE(model, step=1).fit(X, y)
	```
	Eu gosto de utilizar o RFE em modelos que possuem atributos coef_, como SVM, mas ele tambÃ©m funciona bem com Ensembles â€” atente-se aos pontos que falei no tÃ³pico de Feature Importance. Como disse anteriormente, o principal trade-off dessa funÃ§Ã£o Ã© o tempo: se vocÃª tiver um dataset com alta dimensionalidade â€” muitas features e/ou muitos data points â€” esse processo tende a demorar muito.
	VocÃª pode diminuÃ­-lo um pouco ao informar um nÃºmero maior de steps, ou informar uma proporÃ§Ã£o de features para remover. Por exemplo: ao invÃ©s de remover uma feature de cada vez â€” step=1 â€” , vocÃª pode remover 5% das features â€” step=0.05.
- `Sklearn SelectFromModel:` O [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) Ã© uma outra funÃ§Ã£o do sklearn que funciona da seguinte forma: a partir de um modelo (fittado ou nÃ£o), o SFM irÃ¡ remover todas as features que nÃ£o passem do threshold que vocÃª informa em seus argumentos. Essa funÃ§Ã£o soou familiar? De fato, o funcionamento do SelectFromModel Ã© bem parecido com o RFE, contudo, o SFM Ã© menos robusto, jÃ¡ que ele baseia sua seleÃ§Ã£o apenas no threshold informado, enquanto o RFE recursivamente remove as features atravÃ©s de iteraÃ§Ãµes.

---

Antes mesmo de entrarmos na abordagem dos tipos de modelos, como de RegressÃ£o, ClassificaÃ§Ã£o e ClusterizaÃ§Ã£o, quero deixar uma abordagem que me fez enxergar o que de fato Ã© Machine Learning, ou aprendizado de mÃ¡quina.

Aprendizado de mÃ¡quina Ã© a ciÃªncia (e a arte) da programaÃ§Ã£o de computadores de modo que eles possam aprender com os dados. Aqui estÃ¡ uma definiÃ§Ã£o mais generalizada:

> NOTE
> [Aprendizado de mÃ¡quina Ã© o] campo de estudo que possibilita aos computadores a habilidade de aprender sem explicitamente programÃ¡-los.
> -- Arthur Samuel, 1959

Agora um definiÃ§Ã£o mais orientada Ã  minha formaÃ§Ã£o (engenharia)

> NOTE
> Alega-se que um programa de computador aprende pela experiÃªncia E em relaÃ§Ã£o a algum tipo de tarefa T e alguma medida de desempenho P se o seu desempenho em T, conforme medido por P, melhora com a experiÃªncia E.
> -- Tom Mitchell, 1997

Espero que essas definiÃ§Ãµes possam te auxiliar vocÃª no entendimento sobre o que Ã© machine learning, Diante disso vamos falar de modelos?

### Modelos de RegressÃ£o

Antes mesmo de falar dos tipos de modelos de regressÃ£o, Ã© importante relembrar que esses modelos, sÃ£o do tipo de aprendizado supervisionado.
- `Aprendizado supervisionado:` o conjunto de treinamento que vocÃª fornece ao algoritmo inclui as soluÃ§Ãµes desejadas, chamadas de feature e target.

**RegressÃ£o**

A regressÃ£o linear Ã© um mÃ©todo estatÃ­stico utilizado para modelar a relaÃ§Ã£o entre uma variÃ¡vel dependente contÃ­nua e uma ou mais variÃ¡veis independentes.

RegressÃ£o linear simples, tambÃ©m chamada de mÃ­nimos quadrados ordinÃ¡rio (OLS), tenta minimizar a soma dos erros quadrÃ¡ticos.

`Algoritmo de Treinamento da RegressÃ£o Linear`
O calculo da RegressÃ£o Linear para encontrar um ponto que ainda nÃ£o consta na base de dados pode ser definida 
pela expressÃ£o abaixo:

$y = \beta_0 + \beta_1.X + \epsilon$

$y$ $\Rightarrow$ Ã© a variÃ¡vel dependente, ou seja, o valor previsto.

$Beta_0$ $\Rightarrow$ Ã© o coeficiente que intercepta ou que corta o eixo y.
 
$Beta_1$ $\Rightarrow$ Ã© o coeficiente que define a inclinaÃ§Ã£o da reta.

$X$ $\Rightarrow$ Ã© a variÃ¡vel independente, ou seja, a variÃ¡vel preditora.

Para representar a relaÃ§Ã£o entre uma variÃ¡vel dependente ($y$) e uma variÃ¡vel independente ($x$), usamos o modelo
que determina uma linha reta com inclinaÃ§Ã£o $Beta_1$ e intercepto $Beta_0$, com a variÃ¡vel aleatÃ³ria (erro) $\epsilon$, 
considerada normalmente distribuÃ­da com $E(\epsilon) = 0$.

Para simplificar, vamos assumir a premissa de que o valor mÃ©dio da variÃ¡vel $\epsilon$ para um dado valor de $x$ Ã© $0$.

`CALCULO Beta1`
O calcula de inclinaÃ§Ã£o Ã© feito pela expressÃ£o

$\beta_1 =  \frac {n \Sigma xiyi - \Sigma xi \Sigma yi} {n \Sigma xi^2 - (\Sigma xi)^2}$

* $x$ $\Rightarrow$ posiÃ§Ã£o no eixo $X$ do plano cartesiano.
* $y$ $\Rightarrow$ posiÃ§Ã£o no eixo $Y$ do plano cartesiano.
* $i$ $\Rightarrow$ se refere ao i'Ã©simo valor de $X$ e $Y$.
* n $\Rightarrow$ nÃºmero de pares ordenados utilizado na base.
* $\Sigma \Rightarrow$ letra grega que incida somatÃ³rio.

`CALCULO Beta0`
O calcula do intercepto Ã© feito pela expressÃ£o

$\beta_0 = \frac {\Sigma yi - \beta_1 \Sigma xi} {n}$

* $x$ $\Rightarrow$ posiÃ§Ã£o no eixo $X$ do plano cartesiano.
* $y$ $\Rightarrow$ posiÃ§Ã£o no eixo $Y$ do plano cartesiano.
* $i$ $\Rightarrow$ se refere ao i'Ã©simo valor de $X$ e $Y$.
* n $\Rightarrow$ nÃºmero de pares ordenados utilizado na base.
* $\Sigma \Rightarrow$ letra grega que incida somatÃ³rio.

Exemplo Manual de RegressÃ£o Linear

Vamos usar um conjunto de dados fictÃ­cio com 5 pontos. A variÃ¡vel independente \( X \) e a variÃ¡vel dependente \( Y \) sÃ£o:

\[
\begin{array}{|c|c|}
\hline
X & Y \\
\hline
1 & 2 \\
2 & 3 \\
3 & 5 \\
4 & 4 \\
5 & 6 \\
\hline
\end{array}
\]


1. Calcular as MÃ©dias de \( X \) e \( Y \):

\[
\bar{X} = \frac{1 + 2 + 3 + 4 + 5}{5} = 3
\]

\[
\bar{Y} = \frac{2 + 3 + 5 + 4 + 6}{5} = 4
\]

2. Calcular os Coeficientes da RegressÃ£o Linear ( $\beta_0$ e $\beta_1$ ):

$\beta_1$ (slope):

\[
\beta_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
\]

Primeiro, vamos calcular as somas:

\[
\sum (X_i - \bar{X})(Y_i - \bar{Y}) = (1-3)(2-4) + (2-3)(3-4) + (3-3)(5-4) + (4-3)(4-4) + (5-3)(6-4)
\]

\[
= (-2)(-2) + (-1)(-1) + (0)(1) + (1)(0) + (2)(2) = 4 + 1 + 0 + 0 + 4 = 9
\]

\[
\sum (X_i - \bar{X})^2 = (1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2
\]

\[
= (-2)^2 + (-1)^2 + (0)^2 + (1)^2 + (2)^2 = 4 + 1 + 0 + 1 + 4 = 10
\]

Agora podemos calcular $\beta_1$:

\[
\beta_1 = \frac{9}{10} = 0.9
\]

$\beta_0$ (intercept):

\[
\beta_0 = \bar{Y} - \beta_1 \bar{X} = 4 - 0.9 \times 3 = 4 - 2.7 = 1.3
\]

3. EquaÃ§Ã£o da RegressÃ£o Linear:

\[
\hat{Y} = 1.3 + 0.9X
\]

4. PrevisÃ£o dos Valores de \( Y \):

Agora podemos usar a equaÃ§Ã£o da regressÃ£o para prever os valores de \( Y \) para cada valor de \( X \) no nosso conjunto de dados:

| $X$ | $Y$ observado | $\hat{Y}$ |
|-------|------------------|-------------|
| 1     | 2                | $1.3 + 0.9 \times 1 = 2.2$ |
| 2     | 3                | $1.3 + 0.9 \times 2 = 3.1$ |
| 3     | 5                | $1.3 + 0.9 \times 3 = 4.0$ |
| 4     | 4                | $1.3 + 0.9 \times 4 = 4.9$ |
| 5     | 6                | $1.3 + 0.9 \times 5 = 5.8$ |

ConclusÃ£o:

A equaÃ§Ã£o da regressÃ£o linear para os dados fornecidos Ã©:

\[
\hat{Y} = 1.3 + 0.9X
\]


`MÃ©todos de avaliaÃ§Ã£o do modelo`

Avaliar um modelo de regressÃ£o linear Ã© fundamental para entender sua eficÃ¡cia e fazer ajustes conforme necessÃ¡rio. Abaixo estÃ£o as principais formas de avaliar um modelo de regressÃ£o linear utilizando o algoritmo dos mÃ­nimos quadrados ordinÃ¡rios (OLS):

- `RÂ² (Coeficiente de DeterminaÃ§Ã£o): `O RÂ² mede a proporÃ§Ã£o da variÃ¢ncia na variÃ¡vel dependente que Ã© previsÃ­vel a partir das variÃ¡veis independentes. Ele varia de 0 a 1, onde 1 indica que o modelo explica toda a variÃ¢ncia dos dados
	\[
	R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
	\]
	- `O que mede:` ProporÃ§Ã£o da variÃ¢ncia na variÃ¡vel dependente explicada pelas variÃ¡veis independentes.
	- `Quando usar:` Para avaliar a capacidade explicativa do modelo. Ãštil para comparar diferentes modelos com as mesmas variÃ¡veis dependentes.
	- `LimitaÃ§Ãµes:` Pode ser enganoso em modelos com muitas variÃ¡veis independentes.
	- `Para calcular o coeficiente de determinaÃ§Ã£o ğ‘…Â² manualmente, siga estas etapas bÃ¡sicas:`
		1. Calcular a mÃ©dia dos valores observados ( ğ‘¦Ì„ ):
		ğ‘¦Ì„ = 1 ğ‘› âˆ‘ ğ‘–=1 ğ‘› ğ‘¦ğ‘–
		Onde ğ‘¦ğ‘– sÃ£o os valores observados e ğ‘› Ã© o nÃºmero de observaÃ§Ãµes.
		2. Calcular a soma total dos quadrados (SST):
		ğ‘†ğ‘†ğ‘‡ = âˆ‘ ğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ ğ‘¦Ì„)Â²
		3. Calcular a soma dos quadrados dos resÃ­duos (SSE):
		Suponha que vocÃª tenha ajustado um modelo e obteve previsÃµes ğ‘¦Ì‚ğ‘– para cada observaÃ§Ã£o ğ‘¦ğ‘–.
		ğ‘†ğ‘†ğ¸ = âˆ‘ ğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ ğ‘¦Ì‚ğ‘–)Â²
		4. Calcular o coeficiente de determinaÃ§Ã£o ( ğ‘…Â² ):
		ğ‘…Â² = 1 âˆ’ ğ‘†ğ‘†ğ¸ / ğ‘†ğ‘†ğ‘‡
		Aqui estÃ¡ um exemplo de cÃ¡lculo passo a passo usando dados hipotÃ©ticos:
		Suponha que os valores observados ğ‘¦ğ‘– sejam [10, 15, 12, 18, 20] e as previsÃµes do modelo ğ‘¦Ì‚ğ‘– sejam [11, 14, 13, 17, 19].
		Passo 1: Calcular ğ‘¦Ì„:
		ğ‘¦Ì„ = (10 + 15 + 12 + 18 + 20) / 5 = 75 / 5 = 15
		Passo 2: Calcular SST:
		ğ‘†ğ‘†ğ‘‡ = (10 âˆ’ 15)Â² + (15 âˆ’ 15)Â² + (12 âˆ’ 15)Â² + (18 âˆ’ 15)Â² + (20 âˆ’ 15)Â²
				= 25 + 0 + 9 + 9 + 25 = 68
		Passo 3: Calcular SSE:
		ğ‘†ğ‘†ğ¸ = (10 âˆ’ 11)Â² + (15 âˆ’ 14)Â² + (12 âˆ’ 13)Â² + (18 âˆ’ 17)Â² + (20 âˆ’ 19)Â²
				= 1 + 1 + 1 + 1 + 1 = 5
		Passo 4: Calcular ğ‘…Â²:
		ğ‘…Â² = 1 âˆ’ ğ‘†ğ‘†ğ¸ / ğ‘†ğ‘†ğ‘‡ = 1 âˆ’ 5 / 68 = 0.9265

- `Erro MÃ©dio Absoluto (MAE):`MAE Ã© a mÃ©dia das diferenÃ§as absolutas entre os valores previstos e os valores observados. Fornece uma ideia de quÃ£o grandes sÃ£o os erros em mÃ©dia.
	\[
	MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
	\]
	- `O que mede:` MÃ©dia das diferenÃ§as absolutas entre os valores previstos e os valores observados.
	- `Quando usar:` Quando se deseja uma medida simples e intuitiva do erro mÃ©dio.
	- `LimitaÃ§Ãµes:` NÃ£o diferencia entre erros positivos e negativos.
	- `CÃ¡lculo manual do Erro MÃ©dio Absoluto (MAE):`
	**Valores Observados**: [10, 15, 12, 18, 20]
	**PrevisÃµes do Modelo**: [11, 14, 13, 17, 19]
	`Passos do cÃ¡lculo:`
	1. Calcular as diferenÃ§as absolutas entre os valores observados e as previsÃµes:
   \[
	\begin{align*}
   |10 - 11| & = 1 \\
   |15 - 14| & = 1 \\
   |12 - 13| & = 1 \\
   |18 - 17| & = 1 \\
   |20 - 19| & = 1 \\
   \end{align*}
   \]
	2. Somar as diferenÃ§as absolutas:
   \[1 + 1 + 1 + 1 + 1 = 5\]
	3. Dividir a soma pelo nÃºmero de observaÃ§Ãµes ( n = 5  ):
   \[
	MAE = \frac{5}{5} = 1
	\]
	Resultado: Erro MÃ©dio Absoluto (MAE): 1


- `Erro QuadrÃ¡tico MÃ©dio (MSE):`MSE Ã© a mÃ©dia dos quadrados das diferenÃ§as entre os valores previstos e os valores observados. Penaliza erros maiores mais severamente.
	\[
	MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
	\]
	- `O que mede:` MÃ©dia dos quadrados das diferenÃ§as entre os valores previstos e os valores observados.
	- `Quando usar:` Quando se deseja penalizar mais severamente grandes erros.
	- `LimitaÃ§Ãµes:` SensÃ­vel a outliers.
	- `CÃ¡lculo manual do Erro QuadrÃ¡tico MÃ©dio (MSE):`
	   Valores Observados: [10, 15, 12, 18, 20]
	   PrevisÃµes do Modelo: [11, 14, 13, 17, 19]
	   `Passos do cÃ¡lculo:`
		1. Calcular as diferenÃ§as quadrÃ¡ticas entre os valores observados e as previsÃµes:
		\[
		\begin{align*}
		(10 - 11)^2 & = 1 \\
		(15 - 14)^2 & = 1 \\
		(12 - 13)^2 & = 1 \\
		(18 - 17)^2 & = 1 \\
		(20 - 19)^2 & = 1 \\
		\end{align*}
		\]

		2. Somar as diferenÃ§as quadrÃ¡ticas:
		\[
		1 + 1 + 1 + 1 + 1 = 5
		\]

		3. Dividir a soma pelo nÃºmero de observaÃ§Ãµes ( n = 5 ):
		\[
		MSE = \frac{5}{5} = 1
		\]

		Resultado:
		Erro QuadrÃ¡tico MÃ©dio (MSE): 1

- `Raiz do Erro QuadrÃ¡tico MÃ©dio (RMSE):`RMSE Ã© a raiz quadrada do MSE. Ã‰ na mesma unidade da variÃ¡vel dependente, facilitando a interpretaÃ§Ã£o.
	\[
	RMSE = \sqrt{MSE}
	\]
	- `O que mede:` Raiz quadrada do MSE, mantendo a unidade da variÃ¡vel dependente.
	- `Quando usar:` Quando se deseja interpretar o erro na mesma unidade da variÃ¡vel dependente.
	- `LimitaÃ§Ãµes:` SensÃ­vel a outliers.
	- `CÃ¡lculo manual da Raiz do Erro QuadrÃ¡tico MÃ©dio (RMSE):`
	   Valores Observados: [10, 15, 12, 18, 20]
	   PrevisÃµes do Modelo: [11, 14, 13, 17, 19]
	   `Passos do cÃ¡lculo:`
		1. Calcular as diferenÃ§as quadrÃ¡ticas entre os valores observados e as previsÃµes:
		\[
		\begin{align*}
		(10 - 11)^2 & = 1 \\
		(15 - 14)^2 & = 1 \\
		(12 - 13)^2 & = 1 \\
		(18 - 17)^2 & = 1 \\
		(20 - 19)^2 & = 1 \\
		\end{align*}
		\]

		2. Somar as diferenÃ§as quadrÃ¡ticas:
		\[
		1 + 1 + 1 + 1 + 1 = 5
		\]

		3. Dividir a soma pelo nÃºmero de observaÃ§Ãµes ( \( n = 5 \) ) para obter o MSE:
		\[
		MSE = \frac{5}{5} = 1
		\]

		4. Calcular a raiz quadrada do MSE para obter o RMSE:
		\[
		RMSE = \sqrt{1} = 1
		\]

		Resultado
		Raiz do Erro QuadrÃ¡tico MÃ©dio (RMSE): 1

- `Erro Percentual Absoluto MÃ©dio (MAPE):`MAPE Ã© a mÃ©dia dos erros percentuais absolutos entre os valores previstos e os valores observados. Ã‰ uma medida relativa, expressa em porcentagem.
\[
MAPE = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\]
	- `O que mede:` MÃ©dia dos erros percentuais absolutos entre os valores previstos e os valores observados.
	- `Quando usar:` Para medir o erro em termos percentuais.
	- `LimitaÃ§Ãµes:` Pode ser enganoso se houver valores muito prÃ³ximos de zero.
	- `CÃ¡lculo manual do Erro Percentual Absoluto MÃ©dio (MAPE):`
	Valores Observados: [10, 15, 12, 18, 20]
	PrevisÃµes do Modelo: [11, 14, 13, 17, 19]
	`Passos do cÃ¡lculo:`
		1. Calcular os erros percentuais absolutos:
		\[
		\begin{align*}
		\left| \frac{10 - 11}{10} \right| \times 100\% & = 10\% \\
		\left| \frac{15 - 14}{15} \right| \times 100\% & = 6.67\% \\
		\left| \frac{12 - 13}{12} \right| \times 100\% & = 8.33\% \\
		\left| \frac{18 - 17}{18} \right| \times 100\% & = 5.56\% \\
		\left| \frac{20 - 19}{20} \right| \times 100\% & = 5\% \\
		\end{align*}
		\]

		2. Somar os erros percentuais absolutos:
		\[
		10\% + 6.67\% + 8.33\% + 5.56\% + 5\% = 35.56\%
		\]

		3. Dividir a soma pelo nÃºmero de observaÃ§Ãµes ( \( n = 5 \) ):
		\[
		MAPE = \frac{35.56\%}{5} = 7.11\%
		\]

		Resultado
		Erro Percentual Absoluto MÃ©dio (MAPE): 7.11%

- `Ajustado RÂ² (Adjusted RÂ²):`O RÂ² ajustado leva em conta o nÃºmero de variÃ¡veis independentes no modelo. Ã‰ Ãºtil para comparar modelos com diferentes nÃºmeros de variÃ¡veis.
	\[
	R_{adj}^2 = 1 - \left( \frac{SS_{res} / (n - p - 1)}{SS_{tot} / (n - 1)} \right)
	\]
	- `O que mede:` Similar ao RÂ², mas ajusta pela quantidade de variÃ¡veis independentes no modelo.
	- `Quando usar:` Para comparar modelos com diferentes nÃºmeros de variÃ¡veis independentes.
	- `LimitaÃ§Ãµes:` Pode nÃ£o penalizar suficientemente a complexidade do modelo em datasets muito grandes.
	- `CÃ¡lculo manual do coeficiente de determinaÃ§Ã£o ajustado (Adjusted RÂ²):`
	Valores Observados: [10, 15, 12, 18, 20]
	PrevisÃµes do Modelo: [11, 14, 13, 17, 19]
	NÃºmero de ObservaÃ§Ãµes ( n  ): 5
	NÃºmero de Preditores ( k ): 1
	`Passos do cÃ¡lculo:`
		1. MÃ©dia dos valores observados ( $\bar{y}$ ): 15
		2. Soma Total dos Quadrados (SST): 68
		3. Soma dos Quadrados dos ResÃ­duos (SSE): 5
		4. Coeficiente de DeterminaÃ§Ã£o ( $R^2$ ): 0.9265
		5. Coeficiente de DeterminaÃ§Ã£o Ajustado ( $R^2_{ajustado}$ ):
		\[ 
		R^2_{ajustado} = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - k - 1} \right)
		\]
		\[ 
		R^2_{ajustado} = 1 - \left( \frac{(1 - 0.9265)(5 - 1)}{5 - 1 - 1} \right)
		\]
		\[
		R^2_{ajustado} = 1 - \left( \frac{0.0735 \times 4}{3} \right)
		\]
		\[
		R^2_{ajustado} = 1 - \left( \frac{0.294}{3} \right)
		\]
		\[
		R^2_{ajustado} = 1 - 0.098
		\]
		\[
		R^2_{ajustado} = 0.902
		\]

- `Teste F:`O teste F avalia a significÃ¢ncia global do modelo, verificando se pelo menos uma variÃ¡vel independente tem um coeficiente diferente de zero.
	\[
	F = \left( \frac{R^2 / p}{(1 - R^2) / (n - p - 1)} \right)
	\]
	- `O que mede:` SignificÃ¢ncia global do modelo.
	- `Quando usar:` Para verificar se pelo menos uma variÃ¡vel independente tem um efeito significativo na variÃ¡vel dependente.
	- `LimitaÃ§Ãµes:` NÃ£o fornece informaÃ§Ãµes sobre quais variÃ¡veis especÃ­ficas sÃ£o significativas.
	- `CÃ¡lculo manual do Teste F usando a fÃ³rmula:`
	Dados:
	Coeficiente de DeterminaÃ§Ã£o ( $R^2$ ): 0.85
	NÃºmero de Preditores ( $p$ ): 2
	NÃºmero de ObservaÃ§Ãµes ( $n$ ): 10
	`Passos do cÃ¡lculo:`

		1. Calcular a parte superior da fraÃ§Ã£o ( $\frac{R^2}{p}$ ):
		\[
		\frac{R^2}{p} = \frac{0.85}{2} = 0.425
		\]

		2. Calcular a parte inferior da fraÃ§Ã£o ($\frac{1 - R^2}{n - p - 1}$):
		\[
		\frac{1 - R^2}{n - p - 1} = \frac{1 - 0.85}{10 - 2 - 1} = \frac{0.15}{7} \approx 0.0214
		\]

		3. Calcular o valor do Teste F:
		\[
		F = \frac{0.425}{0.0214} \approx 19.86
		\]

		Resultado
		Valor do Teste F: 19.86

- `AnÃ¡lise dos ResÃ­duos:`
	- `GrÃ¡fico de ResÃ­duos vs. Valores Ajustados:` Ajuda a identificar a homocedasticidade e a linearidade.
	- `Histograma dos ResÃ­duos:` Ajuda a verificar a normalidade dos resÃ­duos.
	- `GrÃ¡fico QQ:` Avalia a normalidade dos resÃ­duos.


Um detalhe bastante importante para esse algoritmo Ã© que o cientista de dados, esteja muito ciente de que quanto mais ele diminuir a funÃ§Ã£o de custo 

`Premissas do Modelo de RegressÃ£o Linear`
Para que a regressÃ£o linear produza estimativas vÃ¡lidas e significativas, algumas premissas devem ser atendidas:
- Linearidade: A relaÃ§Ã£o entre a variÃ¡vel dependente e as variÃ¡veis independentes Ã© linear.
- IndependÃªncia: As observaÃ§Ãµes sÃ£o independentes umas das outras.
- Homoscedasticidade: A variÃ¢ncia dos resÃ­duos Ã© constante para todos os nÃ­veis das variÃ¡veis independentes.
- Normalidade dos Erros: Os resÃ­duos do modelo seguem uma distribuiÃ§Ã£o normal.
- AusÃªncia de Multicolinearidade: As variÃ¡veis independentes nÃ£o sÃ£o altamente correlacionadas entre si.

`Quando Utilizar e NÃ£o Utilizar a RegressÃ£o Linear`
*Utilizar Quando:*
- A relaÃ§Ã£o entre as variÃ¡veis Ã© aproximadamente linear.
- O objetivo Ã© interpretar a relaÃ§Ã£o entre a variÃ¡vel dependente e as variÃ¡veis independentes.
- Os dados atendem Ã s premissas do modelo de regressÃ£o linear.

*NÃ£o Utilizar Quando:*
- A relaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© linear.
- Existem outliers significativos que influenciam o modelo.
- HÃ¡ multicolinearidade entre as variÃ¡veis independentes.
- As premissas do modelo de regressÃ£o linear nÃ£o sÃ£o atendidas.

---

- O erro Ã© a diferenÃ§a entre o valor verdadeiro com o valor previsto pelo modelo, essa equaÃ§Ã£o Ã© chamada de funÃ§Ã£o de erro (LOSS).
- `OBS:` Nem sempre o modelo de OLS consegue analisar de forma eficiente os dados, uma situaÃ§Ã£o Ã© quando o dado mostra multi-colinearidade, isto Ã©, quando as variÃ¡veis de entrada estÃ£o correlacionadas entre si e tambÃ©m com a variÃ¡vel de resposta.

2. RegularizaÃ§Ã£o L1, L2, Elastic Net
	- jjkjk
3. Ãrvore de regressÃ£o
4. AnÃ¡lise de resÃ­duos
5. Modelos lineares generalizados (GLM)

**ClassificaÃ§Ã£o**

**Agrupamento**


- Premissas de cada modelo;
- Quando utilizar e nÃ£o utilizar;
- Metodos de regularizaÃ§Ã£o;
- Algoritmo de treinamento;
- Metodos de avaliaÃ§Ã£o do modelo;
- O nome do modelo jÃ¡ o final do metodo do modelo;

---

Olhar as cartas dos fundos, e tentar extrair o mÃ¡ximo de grÃ¡ficos,